{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# vAGI L-KAN train tren Modal.com (GPU A100 40GB)\n",
        "\n",
        "Notebook nay chay train tu xa tren Modal, dung GPU A100 40GB.\n",
        "\n",
        "## Tham khao chinh\n",
        "- Modal quickstart + CLI: https://modal.com/docs/guide\n",
        "- Modal Python SDK (`App`, `function`, `Image`, `Volume`): https://modal.com/docs/reference/modal.App\n",
        "- Modal GPU docs (A100): https://modal.com/docs/guide/gpu\n",
        "- Modal image from registry: https://modal.com/docs/reference/modal.Image#from_registry\n",
        "- Modal volume docs: https://modal.com/docs/reference/modal.Volume\n",
        "\n",
        "## Output\n",
        "- Checkpoint duoc luu trong Modal Volume va co cell de tai ve local.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import pathlib\n",
        "import shlex\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run(cmd: str, check: bool = True, env=None):\n",
        "    print(f\"$ {cmd}\")\n",
        "    completed = subprocess.run(\n",
        "        cmd,\n",
        "        shell=True,\n",
        "        text=True,\n",
        "        capture_output=True,\n",
        "        env=env,\n",
        "    )\n",
        "    if completed.stdout:\n",
        "        print(completed.stdout)\n",
        "    if completed.stderr:\n",
        "        print(completed.stderr)\n",
        "    if check and completed.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed ({completed.returncode}): {cmd}\")\n",
        "    return completed\n",
        "\n",
        "print(\"Python:\", sys.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/vietrix/vagi.git\"\n",
        "BRANCH = \"main\"\n",
        "VOLUME_NAME = \"vagi-lkan-models\"\n",
        "CHECKPOINT_NAME = \"lkan-genesis-a10040gb.safetensors\"\n",
        "TRAIN_STEPS = 5_000\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LEN = 64\n",
        "\n",
        "token_id = os.environ.get(\"MODAL_TOKEN_ID\", \"\").strip()\n",
        "token_secret = os.environ.get(\"MODAL_TOKEN_SECRET\", \"\").strip()\n",
        "if not token_id:\n",
        "    token_id = getpass.getpass(\"MODAL_TOKEN_ID: \").strip()\n",
        "if not token_secret:\n",
        "    token_secret = getpass.getpass(\"MODAL_TOKEN_SECRET: \").strip()\n",
        "\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = token_id\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = token_secret\n",
        "\n",
        "print(\"Repo:\", REPO_URL)\n",
        "print(\"Branch:\", BRANCH)\n",
        "print(\"Volume:\", VOLUME_NAME)\n",
        "print(\"Checkpoint:\", CHECKPOINT_NAME)\n",
        "print(\"Train steps:\", TRAIN_STEPS, \"batch:\", BATCH_SIZE, \"seq_len:\", SEQ_LEN)\n",
        "print(\"MODAL_TOKEN_ID prefix:\", token_id[:6] + \"***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run(f\"{sys.executable} -m pip install -U pip modal\")\n",
        "run(\"modal --version\")\n",
        "run(\"modal token info\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modal_script = r'''\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import urllib.request\n",
        "\n",
        "import modal\n",
        "\n",
        "APP_NAME = \"vagi-lkan-train\"\n",
        "MOUNT_MODELS = \"/models\"\n",
        "DEFAULT_VOLUME = \"vagi-lkan-models\"\n",
        "\n",
        "volume = modal.Volume.from_name(DEFAULT_VOLUME, create_if_missing=True)\n",
        "\n",
        "image = (\n",
        "    modal.Image.from_registry(\"nvidia/cuda:12.4.1-devel-ubuntu22.04\", add_python=\"3.11\")\n",
        "    .apt_install(\"git\", \"curl\", \"build-essential\", \"pkg-config\", \"libssl-dev\", \"ca-certificates\")\n",
        "    .run_commands(\"curl https://sh.rustup.rs -sSf | sh -s -- -y --profile minimal\")\n",
        ")\n",
        "\n",
        "app = modal.App(APP_NAME)\n",
        "\n",
        "def run_cmd(cmd: str, cwd: pathlib.Path | None = None):\n",
        "    print(f\"$ {cmd}\")\n",
        "    subprocess.run(cmd, shell=True, check=True, cwd=str(cwd) if cwd else None)\n",
        "\n",
        "def replace_const(text: str, name: str, value: str, as_str: bool = False) -> str:\n",
        "    pattern = rf\"const {name}: [^=]+ = [^;]+;\"\n",
        "    repl = f\"const {name}: usize = {value};\"\n",
        "    if as_str:\n",
        "        repl = f\"const {name}: &str = \\\"{value}\\\";\"\n",
        "    updated, n = re.subn(pattern, repl, text, count=1)\n",
        "    if n == 0:\n",
        "        raise RuntimeError(f\"Cannot find const {name} in train_lkan.rs\")\n",
        "    return updated\n",
        "\n",
        "def patch_cargo_for_cuda(repo_root: pathlib.Path):\n",
        "    cargo_toml = repo_root / \"kernel\" / \"Cargo.toml\"\n",
        "    text = cargo_toml.read_text(encoding=\"utf-8\")\n",
        "    m = re.search(r\"^candle-core\\\\s*=\\\\s*(.+)$\", text, flags=re.MULTILINE)\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Cannot find candle-core dependency\")\n",
        "    current = m.group(0)\n",
        "    if 'features = [\"cuda\"]' in current:\n",
        "        print(\"candle-core already has cuda feature\")\n",
        "        return\n",
        "    vm = re.search(r\"version\\\\s*=\\\\s*\\\"([^\\\"]+)\\\"\", current) or re.search(r\"\\\"([^\\\"]+)\\\"\", current)\n",
        "    if not vm:\n",
        "        raise RuntimeError(f\"Cannot parse candle-core version from: {current}\")\n",
        "    version = vm.group(1)\n",
        "    new_line = f\"candle-core = {{ version = \\\"{version}\\\", features = [\\\"cuda\\\"] }}\"\n",
        "    cargo_toml.write_text(text.replace(current, new_line), encoding=\"utf-8\")\n",
        "    print(\"patched\", cargo_toml)\n",
        "\n",
        "def patch_train_binary(repo_root: pathlib.Path, train_steps: int, batch_size: int, seq_len: int):\n",
        "    train_rs = repo_root / \"kernel\" / \"src\" / \"bin\" / \"train_lkan.rs\"\n",
        "    src = train_rs.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    src = replace_const(src, \"OUTPUT_PATH\", \"models/lkan-genesis.safetensors\", as_str=True)\n",
        "    src = replace_const(src, \"TRAIN_STEPS\", str(train_steps))\n",
        "    src = replace_const(src, \"BATCH_SIZE\", str(batch_size))\n",
        "    src = replace_const(src, \"SEQ_LEN\", str(seq_len))\n",
        "\n",
        "    src = src.replace(\"hidden_dim: 192,\", \"hidden_dim: 128,\")\n",
        "    src = src.replace(\"in_dim: 192,\", \"in_dim: 128,\")\n",
        "    src = src.replace(\"out_dim: 192,\", \"out_dim: 128,\")\n",
        "\n",
        "    if \"Device::new_cuda(0)\" not in src:\n",
        "        cpu_line = \"let device = Device::Cpu;\"\n",
        "        device_block = \"\"\"let device = match Device::new_cuda(0) {\n",
        "        Ok(dev) => {\n",
        "            println!(\\\"using CUDA device 0\\\");\n",
        "            dev\n",
        "        }\n",
        "        Err(err) => {\n",
        "            println!(\\\"CUDA unavailable ({err}), fallback to CPU\\\");\n",
        "            Device::Cpu\n",
        "        }\n",
        "    };\"\"\"\n",
        "        if cpu_line not in src:\n",
        "            raise RuntimeError(\"Cannot patch device block in train_lkan.rs\")\n",
        "        src = src.replace(cpu_line, device_block)\n",
        "\n",
        "    train_rs.write_text(src, encoding=\"utf-8\")\n",
        "    print(\"patched\", train_rs)\n",
        "\n",
        "def ensure_dataset(repo_root: pathlib.Path):\n",
        "    data_path = repo_root / \"data\" / \"input.txt\"\n",
        "    data_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if data_path.exists():\n",
        "        return data_path\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    print(\"downloading\", url)\n",
        "    urllib.request.urlretrieve(url, data_path)\n",
        "    return data_path\n",
        "\n",
        "@app.function(\n",
        "    image=image,\n",
        "    gpu=\"A100-40GB\",\n",
        "    cpu=8.0,\n",
        "    memory=32768,\n",
        "    timeout=24 * 60 * 60,\n",
        "    volumes={MOUNT_MODELS: volume},\n",
        ")\n",
        "def train(\n",
        "    repo_url: str,\n",
        "    branch: str,\n",
        "    train_steps: int = 5000,\n",
        "    batch_size: int = 32,\n",
        "    seq_len: int = 64,\n",
        "    checkpoint_name: str = \"lkan-genesis-a10040gb.safetensors\",\n",
        "):\n",
        "    os.environ[\"PATH\"] = f\"/root/.cargo/bin:/usr/local/cuda/bin:{os.environ.get('PATH', '')}\"\n",
        "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "    os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64\"\n",
        "\n",
        "    run_cmd(\"nvidia-smi\")\n",
        "    run_cmd(\"rustc --version\")\n",
        "    run_cmd(\"cargo --version\")\n",
        "\n",
        "    workdir = pathlib.Path(\"/root/work\")\n",
        "    repo_root = workdir / \"vagi\"\n",
        "    if repo_root.exists():\n",
        "        shutil.rmtree(repo_root)\n",
        "    workdir.mkdir(parents=True, exist_ok=True)\n",
        "    run_cmd(f\"git clone --depth 1 --branch {branch} {repo_url} {repo_root}\")\n",
        "\n",
        "    patch_cargo_for_cuda(repo_root)\n",
        "    patch_train_binary(repo_root, train_steps=train_steps, batch_size=batch_size, seq_len=seq_len)\n",
        "    ensure_dataset(repo_root)\n",
        "\n",
        "    run_cmd(\"cargo run -p vagi-kernel --release --bin train_lkan\", cwd=repo_root)\n",
        "\n",
        "    candidates = [\n",
        "        repo_root / \"models\" / \"lkan-genesis.safetensors\",\n",
        "        repo_root / \"models\" / \"lkan-gen2.safetensors\",\n",
        "    ]\n",
        "    produced = next((p for p in candidates if p.exists()), None)\n",
        "    if produced is None:\n",
        "        raise RuntimeError(\"Checkpoint not found after training\")\n",
        "\n",
        "    target = pathlib.Path(MOUNT_MODELS) / checkpoint_name\n",
        "    shutil.copy2(produced, target)\n",
        "    volume.commit()\n",
        "\n",
        "    return {\n",
        "        \"checkpoint\": str(target),\n",
        "        \"size_mb\": round(target.stat().st_size / (1024 * 1024), 2),\n",
        "        \"gpu\": \"A100-40GB\",\n",
        "    }\n",
        "\n",
        "@app.local_entrypoint()\n",
        "def main(\n",
        "    repo_url: str = \"https://github.com/vietrix/vagi.git\",\n",
        "    branch: str = \"main\",\n",
        "    train_steps: int = 5000,\n",
        "    batch_size: int = 32,\n",
        "    seq_len: int = 64,\n",
        "    checkpoint_name: str = \"lkan-genesis-a10040gb.safetensors\",\n",
        "):\n",
        "    result = train.remote(\n",
        "        repo_url=repo_url,\n",
        "        branch=branch,\n",
        "        train_steps=train_steps,\n",
        "        batch_size=batch_size,\n",
        "        seq_len=seq_len,\n",
        "        checkpoint_name=checkpoint_name,\n",
        "    )\n",
        "    print(\"TRAIN RESULT:\", result)\n",
        "    print(\"Saved to Modal Volume:\", DEFAULT_VOLUME)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "script_path = pathlib.Path(\"modal_train_lkan.py\")\n",
        "script_path.write_text(modal_script.strip() + \"\\n\", encoding=\"utf-8\")\n",
        "print(\"Wrote:\", script_path.resolve())\n",
        "print(script_path.read_text(encoding=\"utf-8\")[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = (\n",
        "    \"modal run modal_train_lkan.py \"\n",
        "    f\"--repo-url {shlex.quote(REPO_URL)} \"\n",
        "    f\"--branch {shlex.quote(BRANCH)} \"\n",
        "    f\"--train-steps {TRAIN_STEPS} \"\n",
        "    f\"--batch-size {BATCH_SIZE} \"\n",
        "    f\"--seq-len {SEQ_LEN} \"\n",
        "    f\"--checkpoint-name {shlex.quote(CHECKPOINT_NAME)}\"\n",
        ")\n",
        "run(cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run(f\"modal volume ls {VOLUME_NAME} /\")\n",
        "run(\n",
        "    f\"modal volume get {VOLUME_NAME} /{CHECKPOINT_NAME} {CHECKPOINT_NAME}\",\n",
        "    check=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_ckpt = pathlib.Path(CHECKPOINT_NAME)\n",
        "if local_ckpt.exists():\n",
        "    print(\"Downloaded:\", local_ckpt.resolve())\n",
        "    print(\"Size (MB):\", round(local_ckpt.stat().st_size / (1024 * 1024), 2))\n",
        "else:\n",
        "    print(\"Checkpoint chua co local file. Ban co the tai tu Modal Volume tren dashboard.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Done.\")\n",
        "print(\"Neu can train lau hon, tang TRAIN_STEPS roi chay lai cell modal run.\")\n",
        "print(\"Neu can checkpoint ten khac, doi CHECKPOINT_NAME.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
